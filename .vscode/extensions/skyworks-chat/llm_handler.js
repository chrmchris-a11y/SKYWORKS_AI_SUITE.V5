const fs = require('fs');
const path = require('path');

async function handleAskAgentWithLLM(stream, agentName, question, memoryDir, workspaceRoot) {
    stream.markdown(`## ğŸ¤– ${agentName}\n\n`);

    if (!question || question.trim() === '') {
        stream.markdown(`â“ Î Î±ÏÎ±ÎºÎ±Î»Ï Î´ÏÏƒÎµ Î¼Î¹Î± ÎµÏÏÏ„Î·ÏƒÎ·.\n\n`);
        stream.markdown(`Î Î±ÏÎ¬Î´ÎµÎ¹Î³Î¼Î±: \`/ask-sora What is SAIL level for GRC=3?\`\n`);
        return;
    }

    stream.markdown(`**Î•ÏÏÏ„Î·ÏƒÎ·**: ${question}\n\n`);
    stream.markdown(`â³ Running agent reasoning...\n\n`);

    // Check if Azure OpenAI is configured
    const endpoint = process.env.AZURE_OPENAI_ENDPOINT;
    const apiKey = process.env.AZURE_OPENAI_API_KEY;

    const azureConfigured = Boolean(endpoint && apiKey);
    if (!azureConfigured) {
        // Check OpenAI (non-Azure)
        const openaiKey = process.env.OPENAI_API_KEY;
        if (openaiKey) {
            stream.markdown(`ï¿½ Mode: **OpenAI API** (online)\n\n`);
        } else {
            stream.markdown(`ï¿½ğŸŸ  Mode: **Local Reasoner** (offline)\n\n`);
            stream.markdown(`To enable Azure or OpenAI LLM: see \`Tools/TrainingCenter/LLM_SETUP.md\`.\n\n`);
        }
    } else {
        stream.markdown(`ğŸ”· Mode: **Azure OpenAI** (online)\n\n`);
    }

    // Call Python LLM service
    const llmScript = path.join(workspaceRoot, 'Tools', 'TrainingCenter', 'agent_llm.py');
    
    try {
        const { execSync } = require('child_process');
        const result = execSync(`python "${llmScript}" "${agentName}" "${question}"`, {
            encoding: 'utf-8',
            timeout: 60000, // 60s timeout
            maxBuffer: 10 * 1024 * 1024 // 10MB buffer
        });

        const response = JSON.parse(result);

        if (!response.success) {
            stream.markdown(`âŒ **Error**: ${response.error}\n`);
            return;
        }

    // Display LLM response
        stream.markdown(`---\n\n`);
        stream.markdown(response.answer);
        stream.markdown(`\n\n---\n\n`);

    // Visual mode indicator (based on response)
    const openaiKey = process.env.OPENAI_API_KEY;
    const mode = response.mode || (response.model === 'local-reasoner' ? 'local' : (azureConfigured ? 'azure' : (openaiKey ? 'openai' : 'local')));
    const modeBadge = mode === 'azure' ? 'ğŸ”· Azure OpenAI' : (mode === 'openai' ? 'ï¿½ OpenAI API' : 'ï¿½ğŸŸ  Local Reasoner');

        // Metadata
        stream.markdown(`ğŸ“Š **Metadata**:\n`);
    stream.markdown(`- Mode: ${modeBadge}\n`);
    stream.markdown(`- Model: ${response.model}\n`);
        stream.markdown(`- Tokens Used: ${response.tokens_used}\n`);
        stream.markdown(`- Sources Retrieved: ${response.sources.length}\n\n`);

        // If Azure was configured but we still ran local, hint about Python deps
        if (azureConfigured && mode !== 'azure') {
            stream.markdown(`â„¹ï¸ Azure credentials detected, but fell back to local mode.\n`);
            stream.markdown(`Install Python package: \`pip install openai\` and retry. See \`Tools/TrainingCenter/LLM_SETUP.md\`.\n\n`);
        }

        if (response.sources.length > 0) {
            stream.markdown(`ğŸ“ **Knowledge Sources Used**:\n`);
            response.sources.slice(0, 5).forEach(src => {
                stream.markdown(`- \`${src}\`\n`);
            });
        }

    } catch (error) {
        stream.markdown(`âŒ **LLM call failed**: ${error.message}\n\n`);
        stream.markdown(`**Troubleshooting**:\n`);
        stream.markdown(`1. Check Python is installed: \`python --version\`\n`);
        stream.markdown(`2. Install openai package: \`pip install openai\`\n`);
        stream.markdown(`3. Verify env vars: \`$env:AZURE_OPENAI_ENDPOINT\`\n\n`);
        stream.markdown(`ğŸ“– Full guide: \`Tools/TrainingCenter/LLM_SETUP.md\`\n`);
    }
}

async function handleAskAgentKeywordSearch(stream, agentName, question, memoryDir) {
    // Original keyword-based search (fallback)
    const memFile = agentName === 'SORA_Compliance_Agent' 
        ? 'SORA_Compliance_Agent_memory.json'
        : 'Mission_Planning_Agent_memory.json';
    const memPath = path.join(memoryDir, memFile);

    if (!fs.existsSync(memPath)) {
        stream.markdown(`âš ï¸ Agent memory not found. Run training first.\n`);
        return;
    }

    const memory = JSON.parse(fs.readFileSync(memPath, 'utf-8'));

    // Simple keyword search in memory
    const keywords = question.toLowerCase().split(' ').filter(w => w.length > 3);
    const relevantEntries = memory.memory.slice(0, 100).filter(entry => {
        const source = entry.source.toLowerCase();
        const terms = entry.key_terms.map(t => t.toLowerCase());
        return keywords.some(kw => source.includes(kw) || terms.some(t => t.includes(kw)));
    });

    if (relevantEntries.length === 0) {
        stream.markdown(`âš ï¸ Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ ÏƒÏ‡ÎµÏ„Î¹ÎºÎ­Ï‚ Ï€Î»Î·ÏÎ¿Ï†Î¿ÏÎ¯ÎµÏ‚ ÏƒÏ„Î· Î¼Î½Î®Î¼Î· Ï„Î¿Ï… agent.\n\n`);
        stream.markdown(`Î”Î¿ÎºÎ¯Î¼Î±ÏƒÎµ:\n`);
        stream.markdown(`- Î Î¹Î¿ Î³ÎµÎ½Î¹ÎºÎ® ÎµÏÏÏ„Î·ÏƒÎ·\n`);
        stream.markdown(`- Î›Î­Î¾ÎµÎ¹Ï‚-ÎºÎ»ÎµÎ¹Î´Î¹Î¬ Î±Ï€ÏŒ SORA/PDRA/STS\n`);
        return;
    }

    stream.markdown(`âœ… Î’ÏÎ­Î¸Î·ÎºÎ±Î½ **${relevantEntries.length}** ÏƒÏ‡ÎµÏ„Î¹ÎºÎ­Ï‚ Ï€Î·Î³Î­Ï‚ ÏƒÏ„Î· Î¼Î½Î®Î¼Î·.\n\n`);
    stream.markdown(`ğŸ“ **Î£Ï‡ÎµÏ„Î¹ÎºÎ­Ï‚ Ï€Î·Î³Î­Ï‚**:\n`);
    relevantEntries.slice(0, 5).forEach(entry => {
        stream.markdown(`- \`${entry.source}\` (${entry.content_length} chars)\n`);
        if (entry.key_terms.length > 0) {
            stream.markdown(`  - Terms: ${entry.key_terms.slice(0, 5).join(', ')}\n`);
        }
    });

    stream.markdown(`\nğŸ’¡ **Note**: For full expert reasoning, configure Azure OpenAI (see LLM_SETUP.md).\n`);
}

// Export Î³Î¹Î± Ï‡ÏÎ®ÏƒÎ· ÏƒÏ„Î¿ extension.js
module.exports = { handleAskAgentWithLLM };
